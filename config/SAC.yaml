discrete: False

policy:
  learning_rate: 0.003
  gamma: 0.99
  buffer_size: 300000
  learning_starts: 10000
  batch_size: 100
  target_update_interval: 1
  policy_kwargs: 'empty_dict'
  verbose: 1
  net_arch: [300, 400]
  train_freq: 1000
  gradient_steps: 1000
  noise_type: 'normal'
  noise_std: 0.1

  # [plain, entropy, action, next_action, state]
  # [0.0,  0.05,    0.05,   0.1,         0.05 ]

  method: 'entropy'
  ent_coef: 'auto'

buffer:
  prioritized_ensemble: True

learn:
  total_timesteps: 5000000
  log_interval: 100


