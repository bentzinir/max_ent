discrete: True

policy:
  learning_rate: 0000625
  gamma: 0.99
  buffer_size: 100000
  learning_starts: 20000
  batch_size: 32
  temperature: 0.01
  soft: False
  exploration_initial_eps: 0.05
  exploration_final_eps: 0.05
  target_update_interval: 8000
  tau: 1
  policy_kwargs: 'empty_dict'
  verbose: 1

  # [plain, entropy, action, next_action, state]
  # [0.0,  0.05,    0.05,   0.1,         auto ]

  method: 'state'
  target_entropy: 'auto'
  ent_coef: 0.1
  max_ent_frac: 0.95
  max_ent_coef: 10.1
  min_ent_coef: 0.00001

buffer:
  prioritized_ensemble: True

learn:
  total_timesteps: 70000
  log_interval: 10


