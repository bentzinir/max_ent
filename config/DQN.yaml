discrete: True

policy:
  learning_rate: 0.0003
  gamma: 0.99
  buffer_size: 200000
  learning_starts: 50000
  batch_size: 128
  temperature: 0.02
  exploration_initial_eps: 0.05
  exploration_final_eps: 0.05
  target_update_interval: 1000
  policy_kwargs: 'empty_dict'
  verbose: 1

  # [plain, entropy, action, next_action, state]
  # [0.0,  0.05,    0.05,   0.1,         auto ]

  method: 'state'
  target_entropy: 'auto'
  ent_coef: 'auto'
  max_ent_frac: 0.8
  max_ent_coef: 0.1
  min_ent_coef: 0.005

buffer:
  prioritized_ensemble: True

learn:
  total_timesteps: 3000000
  log_interval: 50


