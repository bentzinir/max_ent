discrete: True
off_policy: False

policy:


#  clip_rewards: True
#  clip_param: 0.1
#  vf_clip_param: 10.0
#  entropy_coeff: 0.01
#  train_batch_size: 5000
#  rollout_fragment_length: 100
#  sgd_minibatch_size: 500
#  num_sgd_iter: 10
#  batch_mode: truncate_episodes
#  observation_filter: NoFilter
#  vf_share_layers: true


  learning_rate: 0.0003
  n_steps: 1024  # 100
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: None
  vf_coef: 0.5
  policy_kwargs: 'empty_dict'
  verbose: 1
  method: None
  absolute_threshold: True
  ent_coef: 0.01
  min_red_ent_coef: 0.01
  buffer_size: 50000  # for action model

learn:
  total_timesteps: None
  log_interval: 10


